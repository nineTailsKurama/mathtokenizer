{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RETRO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPorT3g0mGSFXlk9uA0jE7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nineTailsKurama/mathtokenizer/blob/main/notebooks/RETRO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxKJ7hTG2Sou",
        "outputId": "a8705e92-61b6-46b6-9ae1-db2ff957fab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting retro-pytorch\n",
            "  Downloading retro_pytorch-0.2.7-py3-none-any.whl (19 kB)\n",
            "Collecting einops>=0.3\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from retro-pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from retro-pytorch) (4.63.0)\n",
            "Collecting autofaiss\n",
            "  Downloading autofaiss-2.13.2-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from retro-pytorch) (1.21.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 23.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->retro-pytorch) (3.10.0.2)\n",
            "Collecting fsspec>=2022.1.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 63.9 MB/s \n",
            "\u001b[?25hCollecting embedding-reader<2,>=1.2.0\n",
            "  Downloading embedding_reader-1.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting faiss-cpu<2,>=1.7.2\n",
            "  Downloading faiss_cpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 20.9 MB/s \n",
            "\u001b[?25hCollecting dataclasses<1.0.0,>=0.6\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pandas<2,>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from autofaiss->retro-pytorch) (1.3.5)\n",
            "Collecting fire<0.5.0,>=0.4.0\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<8,>=6.0.1 in /usr/local/lib/python3.7/dist-packages (from autofaiss->retro-pytorch) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire<0.5.0,>=0.4.0->autofaiss->retro-pytorch) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire<0.5.0,>=0.4.0->autofaiss->retro-pytorch) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.1.5->autofaiss->retro-pytorch) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.1.5->autofaiss->retro-pytorch) (2.8.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=b049d886b4351ea2ec0cd06b3ee2d92041758fbce777fe16fd18d061ac0c4d4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: fsspec, fire, faiss-cpu, embedding-reader, dataclasses, sentencepiece, einops, autofaiss, retro-pytorch\n",
            "Successfully installed autofaiss-2.13.2 dataclasses-0.6 einops-0.4.1 embedding-reader-1.3.0 faiss-cpu-1.7.2 fire-0.4.0 fsspec-2022.2.0 retro-pytorch-0.2.7 sentencepiece-0.1.96\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.63.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Installing collected packages: huggingface-hub\n",
            "Successfully installed huggingface-hub-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install retro-pytorch\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from retro_pytorch import RETRO"
      ],
      "metadata": {
        "id": "C6J3QkOkEqLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retro = RETRO(\n",
        "    chunk_size = 64,                         # the chunk size that is indexed and retrieved (needed for proper relative positions as well as causal chunked cross attention)\n",
        "    max_seq_len = 2048,                      # max sequence length\n",
        "    enc_dim = 896,                           # encoder model dim\n",
        "    enc_depth = 2,                           # encoder depth\n",
        "    dec_dim = 796,                           # decoder model dim\n",
        "    dec_depth = 12,                          # decoder depth\n",
        "    dec_cross_attn_layers = (3, 6, 9, 12),   # decoder cross attention layers (with causal chunk cross attention)\n",
        "    heads = 8,                               # attention heads\n",
        "    dim_head = 64,                           # dimension per head\n",
        "    dec_attn_dropout = 0.25,                 # decoder attention dropout\n",
        "    dec_ff_dropout = 0.25,                   # decoder feedforward dropout\n",
        "    use_deepnet = True                       # turn on post-normalization with DeepNet residual scaling and initialization, for scaling to 1000 layers\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6tjQlWZzGlm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "QClQEpmnU1Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = torch.randint(0, 20000, (2, 2048 + 1))      # plus one since it is split into input and labels for training\n",
        "retrieved = torch.randint(0, 20000, (2, 32, 2, 128)) # retrieved tokens - (batch, num chunks, num retrieved neighbors, retrieved chunk with continuation)\n",
        "\n",
        "# loss = retro(seq, retrieved, return_loss = True)\n",
        "# loss.backward()"
      ],
      "metadata": {
        "id": "HJPWKI47Tj6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = seq.cuda()\n",
        "retrieved = retrieved.cuda()"
      ],
      "metadata": {
        "id": "4JGjM_DEZpo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retro.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poM4QFxPZt3m",
        "outputId": "d309c641-3c1b-4d39-d6d8-1238a6533db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RETRO(\n",
              "  (token_emb): Embedding(28996, 896)\n",
              "  (pos_emb): Embedding(2048, 896)\n",
              "  (to_decoder_model_dim): Linear(in_features=896, out_features=796, bias=True)\n",
              "  (encoder_output_to_decoder_dim): Linear(in_features=896, out_features=796, bias=True)\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (to_q): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=896, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (to_q): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=896, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=896, out_features=3584, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=3584, out_features=896, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (to_q): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=896, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (to_q): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=896, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=896, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=896, out_features=3584, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=3584, out_features=896, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (rotary_pos_emb): RotaryEmbedding()\n",
              "    (norm_out): Identity()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): None\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): None\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): PostNorm(\n",
              "          (fn): ChunkedCrossAttention(\n",
              "            (cross_attn): Attention(\n",
              "              (dropout): Dropout(p=0.25, inplace=False)\n",
              "              (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (3): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): None\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (4): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): None\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (5): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): PostNorm(\n",
              "          (fn): ChunkedCrossAttention(\n",
              "            (cross_attn): Attention(\n",
              "              (dropout): Dropout(p=0.25, inplace=False)\n",
              "              (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (6): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): None\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (7): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): None\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (8): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): PostNorm(\n",
              "          (fn): ChunkedCrossAttention(\n",
              "            (cross_attn): Attention(\n",
              "              (dropout): Dropout(p=0.25, inplace=False)\n",
              "              (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (9): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): None\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (10): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): None\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (11): ModuleList(\n",
              "        (0): PostNorm(\n",
              "          (fn): Attention(\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "            (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "            (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): PostNorm(\n",
              "          (fn): ChunkedCrossAttention(\n",
              "            (cross_attn): Attention(\n",
              "              (dropout): Dropout(p=0.25, inplace=False)\n",
              "              (to_q): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_k): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_v): Linear(in_features=796, out_features=512, bias=False)\n",
              "              (to_out): Linear(in_features=512, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): PostNorm(\n",
              "          (fn): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=796, out_features=3184, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.25, inplace=False)\n",
              "              (3): Linear(in_features=3184, out_features=796, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((796,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (rotary_pos_emb): RotaryEmbedding()\n",
              "    (norm_out): Identity()\n",
              "  )\n",
              "  (to_logits): Linear(in_features=796, out_features=28996, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = retro(seq, retrieved, return_loss = True)\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "WFYpilqPZyzB",
        "outputId": "90c40f5b-5e40-44ee-dee5-5785526a5907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d53375df2aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/retro_pytorch/retro_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq, retrieved, return_loss)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;31m# decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_retrieved_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# project to logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/retro_pytorch/retro_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context_mask, retrieved)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attn_pos_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/retro_pytorch/retro_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/retro_pytorch/retro_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, context, pos_emb)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mcausal_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 14.76 GiB total capacity; 13.08 GiB already allocated; 223.75 MiB free; 13.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "nw6XxLWXTR-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RETRO training wrapper"
      ],
      "metadata": {
        "id": "uZxwinJ3qQfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from retro_pytorch import RETRO, TrainingWrapper\n",
        "\n",
        "# instantiate RETRO, fit it into the TrainingWrapper with correct settings\n",
        "\n",
        "retro = RETRO(\n",
        "    max_seq_len = 2048,                      # max sequence length\n",
        "    enc_dim = 896,                           # encoder model dimension\n",
        "    enc_depth = 3,                           # encoder depth\n",
        "    dec_dim = 768,                           # decoder model dimensions\n",
        "    dec_depth = 12,                          # decoder depth\n",
        "    dec_cross_attn_layers = (1, 3, 6, 9),    # decoder cross attention layers (with causal chunk cross attention)\n",
        "    heads = 8,                               # attention heads\n",
        "    dim_head = 64,                           # dimension per head\n",
        "    dec_attn_dropout = 0.25,                 # decoder attention dropout\n",
        "    dec_ff_dropout = 0.25                    # decoder feedforward dropout\n",
        ").cuda()\n",
        "\n",
        "# wrapper = TrainingWrapper(\n",
        "#     retro = retro,                                 # path to retro instance\n",
        "#     knn = 2,                                       # knn (2 in paper was sufficient)\n",
        "#     chunk_size = 64,                               # chunk size (64 in paper)\n",
        "#     documents_path = './text_folder',              # path to folder of text\n",
        "#     glob = '**/*.txt',                             # text glob\n",
        "#     chunks_memmap_path = './train.chunks.dat',     # path to chunks\n",
        "#     seqs_memmap_path = './train.seq.dat',          # path to sequence data\n",
        "#     doc_ids_memmap_path = './train.doc_ids.dat',   # path to document ids per chunk (used for filtering neighbors belonging to same document)\n",
        "#     max_chunks = 1_000_000,                        # maximum cap to chunks\n",
        "#     max_seqs = 100_000,                            # maximum seqs\n",
        "#     knn_extra_neighbors = 100,                     # num extra neighbors to fetch\n",
        "#     max_index_memory_usage = '100m',\n",
        "#     current_memory_available = '1G'\n",
        "# )\n",
        "\n",
        "# get the dataloader and optimizer (AdamW with all the correct settings)\n",
        "\n",
        "train_dl = iter(wrapper.get_dataloader(batch_size = 2, shuffle = True))\n",
        "optim = wrapper.get_optimizer(lr = 3e-4, wd = 0.01)\n",
        "\n",
        "# now do your training\n",
        "# ex. one gradient step\n",
        "\n",
        "seq, retrieved = map(lambda t: t.cuda(), next(train_dl))\n",
        "\n",
        "# seq       - (2, 2049)         - 1 extra token since split by seq[:, :-1], seq[:, 1:]\n",
        "# retrieved - (2, 32, 2, 128)   - 128 since chunk + continuation, each 64 tokens\n",
        "\n",
        "loss = retro(\n",
        "    seq,\n",
        "    retrieved,\n",
        "    return_loss = True\n",
        ")\n",
        "\n",
        "# one gradient step\n",
        "\n",
        "loss.backward()\n",
        "optim.step()\n",
        "optim.zero_grad()\n",
        "\n",
        "# do above for many steps, then ...\n",
        "\n",
        "# topk sampling with retrieval at chunk boundaries\n",
        "\n",
        "sampled = wrapper.generate(filter_thres = 0.9, temperature = 1.0) # (1, <2049) terminates early if all <eos>\n",
        "\n",
        "# or you can generate with a prompt, knn retrieval for initial chunks all taken care of\n",
        "\n",
        "prompt = torch.randint(0, 1000, (1, 128))  # start with two chunks worth of sequence\n",
        "sampled = wrapper.generate(prompt, filter_thres = 0.9, temperature = 1.0) # (1, <2049) terminates early if all <eos>\n"
      ],
      "metadata": {
        "id": "HfQOqHTnZvcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RETRO Datasets"
      ],
      "metadata": {
        "id": "88OGJz7hqcDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from retro_pytorch import RETRO, RETRODataset\n",
        "\n",
        "# mock data constants\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "NUM_CHUNKS = 1000\n",
        "CHUNK_SIZE = 64\n",
        "NUM_SEQS = 100\n",
        "NUM_NEIGHBORS = 2\n",
        "\n",
        "def save_memmap(path, tensor):\n",
        "    f = np.memmap(path, dtype = tensor.dtype, mode = 'w+', shape = tensor.shape)\n",
        "    f[:] = tensor\n",
        "    del f\n",
        "\n",
        "# generate mock chunk data\n",
        "\n",
        "save_memmap(\n",
        "    './train.chunks.dat',\n",
        "    np.int32(np.random.randint(0, 8192, size = (NUM_CHUNKS, CHUNK_SIZE + 1)))\n",
        ")\n",
        "\n",
        "# generate nearest neighbors for each chunk\n",
        "\n",
        "save_memmap(\n",
        "    './train.chunks.knn.dat',\n",
        "    np.int32(np.random.randint(0, 1000, size = (NUM_CHUNKS, NUM_NEIGHBORS)))\n",
        ")\n",
        "\n",
        "# generate seq data\n",
        "\n",
        "save_memmap(\n",
        "    './train.seq.dat',\n",
        "    np.int32(np.random.randint(0, 128, size = (NUM_SEQS,)))\n",
        ")\n",
        "\n",
        "# instantiate dataset class\n",
        "# which constructs the sequence and neighbors from memmapped chunk and neighbor information\n",
        "\n",
        "# train_ds = RETRODataset(\n",
        "#     num_sequences = NUM_SEQS,\n",
        "#     num_chunks = NUM_CHUNKS,\n",
        "#     num_neighbors = NUM_NEIGHBORS,\n",
        "#     chunk_size = CHUNK_SIZE,\n",
        "#     seq_len = 2048,\n",
        "#     chunk_memmap_path = './train.chunks.dat',\n",
        "#     chunk_nn_memmap_path = './train.chunks.knn.dat',\n",
        "#     seq_memmap_path = './train.seq.dat'\n",
        "# )\n",
        "\n",
        "# train_dl = iter(DataLoader(train_ds, batch_size = 2))\n",
        "\n",
        "# one forwards and backwards\n",
        "\n",
        "retro = RETRO(\n",
        "    max_seq_len = 2048,                      # max sequence length\n",
        "    enc_dim = 896,                           # encoder model dimension\n",
        "    enc_depth = 3,                           # encoder depth\n",
        "    dec_dim = 768,                           # decoder model dimensions\n",
        "    dec_depth = 12,                          # decoder depth\n",
        "    dec_cross_attn_layers = (1, 3, 6, 9),    # decoder cross attention layers (with causal chunk cross attention)\n",
        "    heads = 8,                               # attention heads\n",
        "    dim_head = 64,                           # dimension per head\n",
        "    dec_attn_dropout = 0.25,                 # decoder attention dropout\n",
        "    dec_ff_dropout = 0.25                    # decoder feedforward dropout\n",
        ").cuda()\n",
        "\n",
        "seq, retrieved = map(lambda t: t.cuda(), next(train_dl))\n",
        "\n",
        "# seq       - (2, 2049)         - 1 extra token since split by seq[:, :-1], seq[:, 1:]\n",
        "# retrieved - (2, 32, 2, 128)   - 128 since chunk + continuation, each 64 tokens\n",
        "\n",
        "loss = retro(\n",
        "    seq,\n",
        "    retrieved,\n",
        "    return_loss = True\n",
        ")\n",
        "\n",
        "loss.backward()\n",
        "\n"
      ],
      "metadata": {
        "id": "F_DHAQU4qdJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}